<!DOCTYPE html>
<!-- saved from url=(0046)file:///C:/Users/thano/Downloads/ThanojCV.html -->
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Thanoj Muddana - CV</title>
  <style>
    @import url('https://fonts.googleapis.com/css2?family=Roboto:wght@400;500;700&display=swap');

    body {
      font-family: 'Roboto', sans-serif;
      background-color: #ffffff;
      margin: 0;
      padding: 20px;
      color: #2d2d2d;
      line-height: 1.6;
    }
    .container {
      max-width: 1000px;
      margin: auto;
      background: #ffffff;
      padding: 30px 50px;
      border-radius: 10px;
      box-shadow: 0 4px 8px rgba(0, 0, 0, 0.05);
    }
    .header {
      background-color: #0f172a; /* Tech-friendly deep gray */
      padding: 20px;
      border-radius: 10px 10px 0 0;
      display: flex;
      align-items: center;
      margin-bottom: 30px;
    }
    .header img {
      border-radius: 50%;
      width: 100px;
      height: 100px;
      object-fit: cover;
      border: 2px solid #fff;
      margin-right: 20px;
    }
    .header .details h1 {
      margin: 0;
      font-size: 32px;
      color: #ffffff;
    }
    .header .details p, .header .details a {
      color: #e2e8f0; /* soft gray */
      font-size: 14px;
    }
    .section h2 {
      background-color: #14b8a6; /* Teal-500 */
      color: #fff;
      padding: 10px;
      border-radius: 4px;
      font-size: 22px;
      margin-bottom: 15px;
    }
    .section {
      margin-bottom: 30px;
    }
    .education-block h3,
    .company-header,
    .project-title {
      color: #0f172a;
    }
    ul.skills li, ul.experience-details li, .education-block p {
      color: #334155;
    }
    .role-location .role {
      color: #1e293b;
    }
    .sub-section-title {
      color: #64748b;
    }
    a {
      color: #14b8a6;
    }
  </style>
</head>
<body>
  <div class="container">
    <!-- Header -->
    <div class="header">
      <img src="./ThanojCV_files/ThanojProfile.jpg" alt="Thanoj Muddana">
      <div class="details">
        <h1>Thanoj Muddana</h1>
        <p><strong>Email:</strong> thanojkrishna.muddana@outlook.com</p>
        <p><strong>GitHub:</strong> <a href="https://github.com/thanojkrishna">github.com/thanojkrishna</a> | <strong>LinkedIn:</strong> <a href="https://www.linkedin.com/in/thanojkrishna">linkedin.com/in/thanojkrishna</a></p>
        <p><strong>Portfolio:</strong> <a href="https://thanojkrishna.github.io/thanojkrishna/">thanojkrishna.github.io/thanojkrishna/</a></p>
      </div>
    </div>
    
    <!-- Professional Summary -->
    <div class="section">
	  <h2>Professional Summary</h2>
	  <p>
        Data Engineering specialist with 6+ years of experience designing cloud-native data platforms, orchestrating scalable ETL pipelines, and enabling enterprise analytics across multi-cloud environments. Proven track record delivering AI-powered solutions by integrating LLMs into data workflows and optimizing pipelines for speed, reliability, and automation. Adept in Python, SQL, Databricks, Snowflake, and Azure Data Factory, with deep expertise in business intelligence, statistical modeling, and cross-functional stakeholder alignment.
      </p>
	</div>

    
    <!-- Education (Placed upfront after Professional Summary) -->
    <div class="section">
	  <h2>Education</h2>

	  <div class="education-block">
		<h3><strong>MS in Statistical Data Science</strong></h3>
		<p><strong>San Francisco State University</strong>, San Francisco, CA | <strong>GPA:</strong> 3.96 | <strong>Graduated:</strong> May 2025</p>
		<p><strong>Coursework:</strong> Data Engineering for Analytics, Cloud Data Pipelines, Statistical Machine Learning, Multivariate Statistics, Distributed Systems</p>
	  </div>

	  <div class="education-block">
		<h3><strong>Bachelor of Science in Mechanical Engineering</strong></h3>
		<p><strong>Vellore Institute of Technology</strong>, Vellore, India | <strong>GPA:</strong> 3.99 | <strong>Graduated:</strong> May 2018</p>
	  </div>
	</div>


    
    <div class="section">
	  <h2>Key Skills</h2>
	  <ul class="skills">
		<li><strong>Programming & Scripting:</strong> Python (pandas, NumPy, requests), R, SQL (T-SQL, PL/SQL), Bash</li>
		<li><strong>ETL & Workflow Orchestration:</strong> SSIS, Azure Data Factory, Apache Airflow, AWS Glue, dbt</li>
		<li><strong>Data Warehousing & Modeling:</strong> Snowflake, Azure Synapse, SQL Server, Dimensional Modeling (Star/Snowflake), SCDs</li>
		<li><strong>Cloud & DevOps:</strong> Azure, AWS, GCP, GitHub Actions (CI/CD), Docker (Airflow, Kafka), Terraform (basic)</li>
		<li><strong>Data Lakes & Formats:</strong> Azure Data Lake Gen2, AWS S3, Delta Lake, JSON, Parquet</li>
		<li><strong>Business Intelligence:</strong> Power BI, Tableau, SAP BO, DAX, Power Query, SSAS Tabular</li>
		<li><strong>Machine Learning & AI:</strong> Scikit-Learn, XGBoost, MLP, OpenAI APIs, LLMs (RAG, embeddings), Vector Search (FAISS)</li>
		<li><strong>Data Quality & Governance:</strong> Data Validation, KPI Documentation, Metadata Audits, SLA Dashboards</li>
		<li><strong>Collaboration & Agile:</strong> Stakeholder Communication, Sprint Planning, RCA, Documentation & Training</li>
	  </ul>
	</div>

	
	<!-- Certifications -->
    <div class="section">
	  <h2>Certifications</h2>
	  <div class="certification-block">
		<ul class="skills">
		  <li>Snowflake SnowPro Core Certification</li>
		  <li>Microsoft Azure Data Engineer Associate (DP-203)</li>
		  <li>Power BI Data Analyst Associate (PL-300)</li>
		  <li>Microsoft Azure Data Fundamentals (DP-900)</li>
		  <li>Microsoft Azure Fundamentals (AZ-900)</li>
		  <li>President’s Leadership Fellows Program – San Francisco State University</li>
		</ul>
	  </div>
	</div>

    
    <!-- Work Experience -->
    <div class="section">
      <h2>Work Experience</h2>
      

    <!-- Stanford Department of Medicine -->
	<div class="company-block">
	  <div class="company-header">
		<div><strong>Stanford Department of Medicine</strong> — AI & Data Science Intern | Stanford, CA | Jan 2024 – Aug 2025</div>
	  </div>
	  <div class="project-title">Project: AI for Clinical Trial Matching & Patient Engagement</div>
	  <ul class="experience-details">
		<li><strong>LLM Integration & RAG:</strong> Built Retrieval-Augmented Generation (RAG) pipelines using OpenAI APIs to match patient profiles with clinical trial inclusion criteria by retrieving structured/unstructured eligibility data from custom vector stores.</li>
		<li><strong>Vector Databases:</strong> Implemented semantic search over medical documents using FAISS-based vector indexing and similarity scoring, enabling faster and more accurate trial-patient mapping.</li>
		<li><strong>Data Preparation & Transformation:</strong> Preprocessed patient demographics, EHR notes, and trial metadata using Python and pandas; normalized inputs for use in NLP pipelines and embedding generation.</li>
		<li><strong>BI & Outcome Analytics:</strong> Designed Power BI dashboards to visualize patient engagement funnels, trial acceptance rates, and RAG output confidence scores to guide physician decision-making.</li>
		<li><strong>Statistical Validation:</strong> Conducted accuracy and sensitivity analysis on RAG predictions vs. baseline models to ensure clinical alignment and safe deployment recommendations.</li>
		<li><strong>Collaboration:</strong> Worked with research scientists and medical coordinators to align LLM-based tools with IRB standards, improving user adoption and model trustworthiness in clinical workflows.</li>
	  </ul>
	</div>



      
	<!-- Accenture -->
	<div class="company-block">
	  <div class="company-header">
		<div><strong>Accenture</strong> — Cloud Data Engineer – Personalization Analytics | Onsite | Jun 2021 – Aug 2023</div>
	  </div>
	  <div class="project-title">Client: AT&T – Digital Personalization Initiative</div>
	  <ul class="experience-details">
		<li><strong>Data Ingestion:</strong> Engineered scalable pipelines using Kafka and Snowpipe to ingest structured and semi-structured data (~150M daily JSON events) from clickstream, campaign logs, and transactional systems into Snowflake and ADLS.</li>
		<li><strong>Data Transformation:</strong> Developed modular dbt models (staging → mart layers) for ELT processing in Snowflake, integrating ML outputs and supporting attribution metrics across marketing and sales funnels.</li>
		<li><strong>Workflow Orchestration:</strong> Automated ELT and reporting jobs using Apache Airflow DAGs, handling task scheduling, dependency management, and failure alerts to ensure SLA compliance.</li>
		<li><strong>Data Modeling:</strong> Designed and maintained star schema models including fact_clickstream, fact_sales, and dim_content to support real-time KPI dashboards and A/B test reporting.</li>
		<li><strong>Data Quality & Governance:</strong> Applied CDC and deduplication logic during ingestion and transformation; enforced schema validation and integrated business rules into staging layer pipelines.</li>
		<li><strong>BI Enablement:</strong> Optimized Power BI semantic models using DAX Studio and Tabular Editor; created dashboards tracking conversions, churn, and campaign lift with Row-Level and Object-Level Security.</li>
		<li><strong>DevOps & CI/CD:</strong> Implemented GitHub Actions for version-controlled deployment of dbt models and Power BI datasets, enabling seamless environment promotion and rollback.</li>
		<li><strong>Collaboration:</strong> Worked in Agile sprints with cross-functional teams (DevOps, analysts, data scientists) to align pipeline logic with tagging, experimentation, and business goals.</li>
	  </ul>
	</div>


      
      <!-- Infosys -->
	<div class="company-block">
	  <div class="company-header">
		<div><strong>Infosys Limited</strong> — Data Engineer – HR & Sales Analytics | Onsite | Jun 2018 – Jun 2021</div>
	  </div>
	  <div class="project-title">Project: Global HR & Sales Platforms</div>
	  <ul class="experience-details">
		<li><strong>Data Warehousing & Modeling:</strong> Designed star schema data models with fact/dimension tables, surrogate keys, and SCD Type 2 logic using SQL Server and Azure Synapse, improving query performance by 55% and enabling secure, regional reporting across 59 countries.</li>
		<li><strong>ETL Pipeline Engineering:</strong> Reengineered SSIS pipelines with staging layers, change tracking, and optimized T-SQL logic; reduced master job runtime by 85% and improved reliability of daily refresh cycles.</li>
		<li><strong>Data Quality & Validation:</strong> Built pre-load staging validation logic to enforce null handling, referential integrity, and field constraints — ensuring schema compliance and trusted data in downstream BI reporting.</li>
		<li><strong>Cloud Modernization:</strong> Integrated legacy SSIS workflows with Azure Data Factory, Synapse, and Azure SQL to support cloud-native, modular transformation pipelines as part of BI modernization roadmap.</li>
		<li><strong>BI & Semantic Layer:</strong> Developed centralized Power BI datasets with SSAS Tabular models, DAX measures, and secure access via Row-Level and Object-Level Security; reduced ad hoc reporting requests by 60% through self-service analytics adoption.</li>
		<li><strong>Observability & Governance:</strong> Maintained documentation for KPIs, refresh schedules, and field-level definitions; created SLA dashboards using SQL Agent logs and performed root cause analysis (RCA) on reporting failures to improve reliability and audit readiness.</li>
	  </ul>
	</div>

    
    <!-- Academic Projects -->
    <div class="section">
	  <h2>Academic Projects</h2>

	  <!-- Bay Area Traffic Prediction -->
	  <div class="project-block">
		<h3>Bay Area Traffic Prediction</h3>
		<ul class="experience-details">
		  <li><strong>Goal:</strong> Forecast hourly traffic volume on Bay Area expressways using real-time and historical sensor data.</li>
		  <li><strong>Tools & Tech:</strong> R, Power BI, Caltrans PeMS (~15M rows → 250K processed); regression modeling and EDA.</li>
		  <li><strong>Approach:</strong> Engineered peak-hour and temporal features; applied regression models (XGBoost, Lasso, Ridge); XGBoost achieved R² = 0.95, RMSE = 427.5.</li>
		  <li><strong>Impact:</strong> Identified key predictors like occupancy and speed; proposed dashboard insights to inform traffic signal optimization and urban safety planning.</li>
		</ul>
	  </div>

	  <!-- Income Classification -->
	  <div class="project-block">
		<h3>Income Classification</h3>
		<ul class="experience-details">
		  <li><strong>Goal:</strong> Classify whether an individual's income is >50K using demographic data and statistical models.</li>
		  <li><strong>Tools & Tech:</strong> Python, Scikit-Learn, Pandas, SMOTE, Seaborn, Adult Income dataset.</li>
		  <li><strong>Approach:</strong> Applied data cleaning, categorical encoding, and SMOTE balancing; tested ensemble, tree-based, and neural models.</li>
		  <li><strong>Impact:</strong> Random Forest achieved 89.1% accuracy, AUC = 0.92; identified key features like education level and capital gains for income prediction.</li>
		</ul>
	  </div>

	  <!-- Movie Recommendation System -->
	  <div class="project-block">
		<h3>Movie Recommendation System</h3>
		<ul class="experience-details">
		  <li><strong>Goal:</strong> Build a hybrid movie recommender using content-based filtering and collaborative algorithms.</li>
		  <li><strong>Tools & Tech:</strong> Python, Scikit-Learn, Surprise (SVD), TF-IDF, Cosine Similarity, Kaggle Movies dataset.</li>
		  <li><strong>Approach:</strong> Used metadata features (genre, keywords) for content filtering; applied SVD for collaborative filtering; combined both for hybrid output.</li>
		  <li><strong>Impact:</strong> Achieved RMSE ~0.874; proposed a scalable framework for OTT platforms to enhance user retention via personalization.</li>
		</ul>
	  </div>

	  <!-- Indian Premier League (IPL) Prediction -->
	  <div class="project-block">
		<h3>Indian Premier League (IPL) Prediction</h3>
		<ul class="experience-details">
		  <li><strong>Goal:</strong> Predict IPL match outcomes and player value insights using classification models and auction analytics.</li>
		  <li><strong>Tools & Tech:</strong> Python, Scikit-Learn, Power BI, IPL match and auction data (2008–2022).</li>
		  <li><strong>Approach:</strong> Built features on player impact, venue bias, and head-to-head stats; implemented Gradient Boosting, SVM, Random Forest; tuned via grid search.</li>
		  <li><strong>Impact:</strong> Achieved 86% accuracy; proposed auction strategy recommendations and player valuation insights for team management.</li>
		</ul>
	  </div>
	</div>

</div>


</body></html>